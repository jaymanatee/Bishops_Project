# ðŸ“¢ Automatic Alert Generation from News and Social Media

This repository implements a multimodal pipeline for automatic alert generation from tweets and associated images. The system combines Named Entity Recognition (NER) and Sentiment Analysis (SA) with Image Captioning, and uses a Large Language Model (LLM) to produce context-aware alerts.

Developed for the final project of the **Deep Learning + NLP (DL+NLP)** course (Academic Year 2024/2025).

---

### ðŸ§  Overview

The system processes textual and visual information from social media posts (specifically tweets from Twitter from 2015 and 2017), detects key entities and sentiment, and produces alerts automatically. The final alert is generated by a generative model (DeepSeek LLM) from the outputs of the NER and SA modules.
Final model: https://drive.google.com/file/d/1BloRG8GjU-FkeTahLej-N6U25gaR8omv/view?usp=sharing
Images (extract in data folder to match folder hierarchy): https://1drv.ms/u/c/e79fddf5998bd0e8/EfWRIvgmlItPjCuOW1BQqfoBms_PRX_pA53kiKP9wLaiYA?e=CKf5vJ

---

### ðŸ”„ Pipeline

The project follows a standard ML pipeline divided into several stages:

#### 1. ðŸ” Data Collection & Preprocessing (`src/process_data.py`)
- Collected tweet datasets (2015 & 2017) with pre-annotated NER labels.
- Extracted tweet IDs and matched them with associated image files.
- Generated image captions using a pre-trained image captioning model.
- Created consolidated CSV files for training, validation, and testing, including:
  - tweet ID  
  - tweet text  
  - image caption  
  - NER labels  
  - SA label  
- Efficient checks avoid reprocessing if CSVs already exist (especially helpful due to slow captioning).

#### 2. ðŸ§© Dataset Loading (`src/data.py`)
- Uses a BERT tokenizer to tokenize both the tweet and the generated caption (concatenated).
- Converts the data into PyTorch `Dataset` and `DataLoader` objects for:
  - Train  
  - Validation  
  - Test

#### 3. ðŸ—ï¸ Model Training (`src/train.py`)
- Trains a custom LSTM-based architecture that jointly performs:
  - Named Entity Recognition (NER)  
  - Sentiment Analysis (SA)
- Uses a combined loss function to optimize both tasks simultaneously.
- Stores trained models in the `models/` directory.

#### 4. ðŸ§ª Evaluation & Alert Generation (`src/evaluate.py`)
- Evaluates the model on the test set.
- Generates alerts using DeepSeek's LLM based on the output of the NER+SA model.
- For each tweet + caption, the system forms a prompt using:

```
[Tweet] + [Caption] â†’ [Entities] + [Sentiment] â†’ Alert
```

---

### ðŸ—‚ Folder Structure

```
Bishops_Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ twitter2015/               # Tweets from 2015 with original annotations (text, entities, sentiment)
â”‚   â”œâ”€â”€ twitter2015_images/        # Images associated with 2015 tweets
â”‚   â”œâ”€â”€ twitter2017/               # Tweets from 2017 with original annotations
â”‚   â”œâ”€â”€ twitter2017_images/        # Images associated with 2017 tweets
â”‚   â”œâ”€â”€ csv/                       # Processed files ready for the model (text + caption + labels)
â”‚   â”‚   â”œâ”€â”€ train.txt
â”‚   â”‚   â”œâ”€â”€ test.txt
â”‚   â”‚   â”œâ”€â”€ valid.txt
â”‚
â”œâ”€â”€ models/                        # Saved model checkpoints
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py                  # Multitask model architecture (NER + SA) and DeepSeek wrapper
â”‚   â”œâ”€â”€ train_functions.py         # Helper functions for training, validation, and testing
â”‚   â”œâ”€â”€ utils.py                   # General utilities (e.g., captioning, logging)
â”‚   â”œâ”€â”€ process_data.py            # Preprocessing: merges text + captions + labels and saves CSVs
â”‚   â”œâ”€â”€ data.py                    # Tokenization and PyTorch Dataset/DataLoader definitions
â”‚   â”œâ”€â”€ train.py                   # Model training script
â”‚   â”œâ”€â”€ evaluate.py                # Model evaluation and automatic alert generation with the LLM
â”‚   â”œâ”€â”€ .env                       # Environment variables for external API usage
â”‚   â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt              # Project dependencies
â””â”€â”€ README.md
```

---

### âš™ï¸ Setup Instructions

```bash
# 1. Clone this repository
git clone https://github.com/jaymanatee/Bishops_Project
cd Bishops_Project

# 2. Create a virtual environment (optional but recommended)
python3 -m venv bishops_env
source bishops_env/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
```

---

### ðŸš€ Usage

#### ðŸ”§ Preprocess Data
```bash
python -m src.process_data
```

#### ðŸ‹ï¸â€â™‚ï¸ Train the Model
```bash
python -m src.train
```

#### ðŸ“Š Evaluate the Model & Generate Alerts
```bash
python -m src.evaluate
```

âš ï¸ Make sure image files and raw tweet datasets are in `data/` and follow the expected format.
To run the pipeline on new input, add the new tweet/image pair into the same structure and re-run `process_data.py`.

---

### ðŸ” Input Format

A tweet and its associated image must include:
- Tweet ID
- Raw tweet text
- NER labels
- Associated image file

Preprocessing merges these into one CSV per split (train, val, test):

```csv
id,tweet,caption,ner,sentiment
```

---

### ðŸ“Ž Alert Example

**Tweet:**  
"Governor McCrory presents NC flag to Ruger CEO Mike Fifer at jobs announcement in Mayodan"

**Caption:**  
"Man holding a flag beside a podium with microphones."

**NER:**  
Governor McCrory [PER], Ruger [ORG], Mike Fifer [PER]  
**SA:** Positive

â†’ **Generated Alert:**  
```
ALERT -> Governor McCrory presents NC flag to Ruger CEO Mike Fifer at jobs announcement in Mayodan
```

---

### ðŸ’¡ Notes

- We use a joint model for NER and SA to simplify the architecture and allow multi-task learning.
- Image captioning is done using a pre-trained model (not trained from scratch).
- Alerts are generated using prompt-based interaction with DeepSeek LLM.
- The pipeline is modular: you can plug in other LLMs or fine-tune your own captioning model.

---

### ðŸ‘¥ Team

The project was developed by a collaborative student team from the IMAT program. The team worked cohesively, with each member contributing significantly across different aspects of the project. While all members contributed to various tasks, their specific areas of focus were:

- **Enrique FernÃ¡ndez Baillo** and **Jacobo BanÃºs** focused on research aimed at improving the models, especially the optimization of the NER and sentiment analysis components.
- **Almudena Garrido** concentrated on image captioning, embeddings, and alert generation, ensuring that visual and textual data were effectively integrated into the pipeline.
- **SofÃ­a Negueruela** took charge of the data preprocessing and organization of the overall project structure, including maintaining the scientific paper's order and clarity.

Despite the division of tasks, the team worked closely and collaboratively, enriching each otherâ€™s contributions to create a seamless and effective system for automatic alert generation from tweets and associated images.
